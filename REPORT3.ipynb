{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 【ラビットチャレンジ】深層学習前編（day1,day2）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. ニューラルネットワーク全体像\n",
    "\n",
    "<img src='NN_1.jpg'>\n",
    "\n",
    "機械学習のモデル\n",
    "* 識別モデル\n",
    "* 生成モデル\n",
    "\n",
    "\n",
    "\n",
    "|  モデル  |  識別  | 生成 |\n",
    "| ------- | ---- | ---- |\n",
    "|  目的  |  データを目的のクラスに分類する  | 特定のクラスのデータを生成する |\n",
    "|  例 | 動物の画像から種別を分類 | 犬らしい画像の生成 |\n",
    "|  計算結果  |  $p(c_k|x)$   | $ p(x|C_k) $|\n",
    "| 具体的なモデル | SVM、NN | VAE, GAN |\n",
    "\n",
    "\n",
    "万能近似定理\n",
    "\n",
    "ニューラルネットワークのようなネットワークによりどんな関数でも近似できる。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 確認テスト\n",
    "<img src='DNN_1.jpg'>\n",
    "\n",
    "##  1.入力～中間層\n",
    "\n",
    "\n",
    "<img src='INPUT_1.jpg'>\n",
    "\n",
    "入力層では、ニューラルネットワークに何等かの数値を入力する層\n",
    "\n",
    "\n",
    "ここでは、n個の入力ノードから $ (x_1, x_2, \\dots x_n) $　を受けとる。\n",
    "入力層から入った入力には、入力ノード毎の重み $ w_i $が乗算され、バイアス $ b_i $を加え、\n",
    "中間層に伝達する。\n",
    "\n",
    "すなわち、\n",
    "$$\n",
    "z = \\sum ( w_i x_i + b_i )\n",
    "$$\n",
    "が中間層の入力となる。\n",
    "\n",
    "\n",
    "ニューラルネットワークの学習では、$\\mathbf{W}$, $b$を調整してゆくことになる。\n",
    "\n",
    "\n",
    "### 確認テスト\n",
    "\n",
    "<img src='INPUT_2.jpg'>\n",
    "\n",
    "### 確認テスト\n",
    "<code>\n",
    "    u1=np.dot(x,W1)+b1\n",
    "</code>\n",
    "\n",
    "#### 実装実習\n",
    "\n",
    "3層のネットワークの処理部分\n",
    "\n",
    "<code>\n",
    "    # 1層の総入力\n",
    "    u1 = np.dot(x, W1) + b1\n",
    "    # 1層の総出力\n",
    "    z1 = functions.relu(u1)\n",
    "    # 2層の総入力\n",
    "    u2 = np.dot(z1, W2) + b2\n",
    "    # 2層の総出力\n",
    "    z2 = functions.relu(u2)\n",
    "    # 出力層の総入力\n",
    "    u3 = np.dot(z2, W3) + b3\n",
    "    # 出力層の総出力\n",
    "    y = u3\n",
    "</code>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.活性化関数\n",
    "\n",
    "次の層への出力の大きさを決める。\n",
    "* シグモイド、tanh 等 : そのノードの寄与する割合を決定する\n",
    "* 恒等関数等 : 値の強弱を決める。\n",
    "\n",
    "活性化関数 は 非線形関数であることが重要\n",
    "\n",
    "線形性とは\n",
    "\n",
    "加法性\n",
    "$$\n",
    "f(x+y) = f(x) + f(y)\n",
    "$$\n",
    "\n",
    "斉次性\n",
    "$$\n",
    "f(k x) = k f(x)\n",
    "$$\n",
    "を満たすこと。\n",
    "\n",
    "非線形の関数を用いることで、多彩な表現を記述することが可能となる。\n",
    "\n",
    "線形関数は、多数のニューロンを組み合わせなくても、表現可能\n",
    "となってしまうため、意味がない。\n",
    "\n",
    "#### 確認テスト\n",
    "\n",
    "<img src=\"LINEAR_1.jpg\">\n",
    "\n",
    "### 中間層\n",
    "\n",
    "中間層と出力層で使われる活性化関数が異なる。\n",
    "\n",
    "中間層で使われる主な活性化関数は以下の通り\n",
    "\n",
    "* ステップ関数　もはや使われない\n",
    "* シグモイド\n",
    "* ReLU関数　勾配消失解消\n",
    "\n",
    "#### 確認テスト\n",
    "\n",
    "<code>\n",
    "   z = functions.relu(u)   \n",
    "</code>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 実装演習\n",
    "\n",
    "代表的な活性化関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ebe3f42548>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def step_function(x):\n",
    "    return np.where(x>0, 1.0, 0.0)\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "x=np.linspace(-5,5,100)\n",
    "\n",
    "plt.ylim([0,2])\n",
    "plt.plot(x,step_function(x),label='STEP')\n",
    "plt.plot(x,sigmoid(x),label='SIGMOID')\n",
    "plt.plot(x,relu(x),label='ReLU')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認テスト\n",
    "\n",
    "<code>\n",
    "   z = functions.relu(u)   \n",
    "</code>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.出力層\n",
    "\n",
    "人間が欲しいデータを最終的に出してほしいものを出力\n",
    "\n",
    "分類問題の場合、各クラスに属する確率\n",
    "\n",
    "\n",
    "### 活性化関数\n",
    "* ソフトマックス関数\n",
    "* 恒等写像\n",
    "* シグモイド関数\n",
    "\n",
    "出力層で得られた結果を教師データと比較し、教師データに近づけるように重みを調整する。\n",
    "\n",
    "#### ソフトマックス関数\n",
    "$$\n",
    "y_i = { e^x_i \\over \\sum_j e^x_j }\n",
    "$$\n",
    "* 多値分類。\n",
    "* 0 ～　1の値の配列。合計で1 となる。\n",
    "* それぞれのクラスに属する確率（ワンホットベクトル表現)\n",
    "\n",
    "例えば\n",
    "犬、猫、ネズミを判別するニューラルネットの出力が\n",
    "$$\n",
    "y = [0.9, 0.08, 0.02]\n",
    "$$\n",
    "であった場合、犬である確率 90%, 猫である確率 8%、ネズミである確率 2% \n",
    "という具合に読み替えられる。\n",
    "\n",
    "#### 恒等写像\n",
    "$$\n",
    "y_i = x_i\n",
    "$$\n",
    "\n",
    "#### シグモイド\n",
    "\n",
    "$$\n",
    "y = \\sigma (x)\n",
    "$$\n",
    "* 2値分類。\n",
    "* 0 ～　1の値をとる。\n",
    "* 真である確率\n",
    "\n",
    "### 誤差関数\n",
    "\n",
    "教師データとネットワークの出力データを比較し、その違いの大きさを測る。\n",
    "\n",
    "#### 二乗誤差\n",
    "\n",
    "$$\n",
    "E_n = \\frac{1}{2} \\sum_i (y_i - d_i)^2\n",
    "$$\n",
    "教師データ$d_i$とどれだか離れているか？\n",
    "\n",
    "#### 交差エントロピー\n",
    "\n",
    "$$\n",
    "E_n = - \\sum_i d_i \\log y_i\n",
    "$$\n",
    "教師データの分布$d_i$とどれだけ似ているか？\n",
    "\n",
    "#### 比較\n",
    "\n",
    "|   問題の種類 |  回帰  |二値分類  |多値分類  |\n",
    "| ---- | ---- |---- |---- |\n",
    "| 活性化関数  |  恒等写像  |シグモイド|ソフトマックス|\n",
    "|  誤差関数  |  二乗誤差  |交差エントロピー|交差エントロピー|\n",
    "\n",
    "\n",
    "#### 確認テスト\n",
    "\n",
    "誤差伝搬では、誤差を微分した値が前方の層に伝えられる。\n",
    "\n",
    "$ E_n(w) $ の微分は\n",
    "\n",
    "$$\n",
    "{\\partial E \\over \\partial y} = (y-d)\n",
    "$$\n",
    "となり、1/2を乗ずる必要がなくなり、記述が簡単になる。\n",
    "それ以上の本質的な意味はない。\n",
    "\n",
    "### 実装演習1_1_forward_propagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'common'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ca4fb6dbbde9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\emo\\\\Google Drive\\\\DNN_code_colab_lesson_1_2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'common'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\emo\\\\Google Drive\\\\DNN_code_colab_lesson_1_2')\n",
    "import numpy as np\n",
    "from common import functions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_vec(text, vec):\n",
    "    print(\"*** \" + text + \" ***\")\n",
    "    print(vec)\n",
    "    #print(\"shape: \" + str(x.shape))\n",
    "    print(\"\")\n",
    "\n",
    "# ウェイトとバイアスを設定\n",
    "# ネートワークを作成\n",
    "def init_network():\n",
    "    print(\"##### ネットワークの初期化 #####\")\n",
    "\n",
    "    network = {}\n",
    "    network['W1'] = np.array([\n",
    "        [0.1, 0.3, 0.5],\n",
    "        [0.2, 0.4, 0.6]\n",
    "    ])\n",
    "\n",
    "    network['W2'] = np.array([\n",
    "        [0.1, 0.4],\n",
    "        [0.2, 0.5],\n",
    "        [0.3, 0.6]\n",
    "    ])\n",
    "\n",
    "    network['b1'] = np.array([0.1, 0.2, 0.3])\n",
    "    network['b2'] = np.array([0.1, 0.2])\n",
    "    \n",
    "    print_vec(\"重み1\", network['W1'])\n",
    "    print_vec(\"重み2\", network['W2'])\n",
    "    print_vec(\"バイアス1\", network['b1'])\n",
    "    print_vec(\"バイアス2\", network['b2'])\n",
    "\n",
    "    return network\n",
    "\n",
    "# 順伝播\n",
    "def forward(network, x):\n",
    "    print(\"##### 順伝播開始 #####\")\n",
    "\n",
    "    W1, W2 = network['W1'], network['W2']\n",
    "    b1, b2 = network['b1'], network['b2']\n",
    "    \n",
    "    u1 = np.dot(x, W1) + b1\n",
    "    z1 = functions.relu(u1)\n",
    "    u2 = np.dot(z1, W2) + b2\n",
    "    y = functions.softmax(u2)\n",
    "    \n",
    "    print_vec(\"総入力1\", u1)\n",
    "    print_vec(\"中間層出力1\", z1)\n",
    "    print_vec(\"総入力2\", u2)\n",
    "    print_vec(\"出力1\", y)\n",
    "    print(\"出力合計: \" + str(np.sum(y)))\n",
    "\n",
    "    return y, z1\n",
    "\n",
    "# 誤差逆伝播\n",
    "def backward(x, d, z1, y):\n",
    "    print(\"\\n##### 誤差逆伝播開始 #####\")\n",
    "\n",
    "    grad = {}\n",
    "\n",
    "    W1, W2 = network['W1'], network['W2']\n",
    "    b1, b2 = network['b1'], network['b2']\n",
    "    #  出力層でのデルタ\n",
    "    delta2 = functions.d_sigmoid_with_loss(d, y)\n",
    "    #  b2の勾配\n",
    "    grad['b2'] = np.sum(delta2, axis=0)\n",
    "    #  W2の勾配\n",
    "    grad['W2'] = np.dot(z1.T, delta2)\n",
    "    #  中間層でのデルタ\n",
    "    delta1 = np.dot(delta2, W2.T) * functions.d_relu(z1)\n",
    "    # b1の勾配\n",
    "    grad['b1'] = np.sum(delta1, axis=0)\n",
    "    #  W1の勾配\n",
    "    grad['W1'] = np.dot(x.T, delta1)\n",
    "        \n",
    "    print_vec(\"偏微分_dE/du2\", delta2)\n",
    "    print_vec(\"偏微分_dE/du2\", delta1)\n",
    "\n",
    "    print_vec(\"偏微分_重み1\", grad[\"W1\"])\n",
    "    print_vec(\"偏微分_重み2\", grad[\"W2\"])\n",
    "    print_vec(\"偏微分_バイアス1\", grad[\"b1\"])\n",
    "    print_vec(\"偏微分_バイアス2\", grad[\"b2\"])\n",
    "\n",
    "    return grad\n",
    "    \n",
    "# 訓練データ\n",
    "x = np.array([[1.0, 5.0]])\n",
    "# 目標出力\n",
    "d = np.array([[0, 1]])\n",
    "#  学習率\n",
    "learning_rate = 0.01\n",
    "network =  init_network()\n",
    "y, z1 = forward(network, x)\n",
    "\n",
    "# 誤差\n",
    "loss = functions.cross_entropy_error(d, y)\n",
    "\n",
    "grad = backward(x, d, z1, y)\n",
    "for key in ('W1', 'W2', 'b1', 'b2'):\n",
    "    network[key]  -= learning_rate * grad[key]\n",
    "\n",
    "print(\"##### 結果表示 #####\")    \n",
    "\n",
    "\n",
    "print(\"##### 更新後パラメータ #####\") \n",
    "print_vec(\"重み1\", network['W1'])\n",
    "print_vec(\"重み2\", network['W2'])\n",
    "print_vec(\"バイアス1\", network['b1'])\n",
    "print_vec(\"バイアス2\", network['b2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.勾配降下法\n",
    "\n",
    "### 深層学習の目的\n",
    "\n",
    "学習を行い誤差を最小にする。（誤差 $E(w)$を最小にする$w$を見つける\n",
    "\n",
    "$$\n",
    "{\\mathbf w}^{(t+1)}={\\mathbf w}^(t) - \\epsilon \\nabla E \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla E = {\\partial E \\over \\partial {\\mathbf w}} =  [ {\\partial E \\over \\partial w_1}, \\dots  {\\partial E \\over \\partial w_M}]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "のように、誤差 ${\\mathbf w}$を逐次更新する。\n",
    "\n",
    "<img src=\"GRADIENT_2.jpg\">\n",
    "\n",
    "$ E(x) $ の接線方向に x を更新することで、極小値に近づく。\n",
    "\n",
    "$ \\epsilon $ が更新する割合を決めるマジックナンバー\n",
    "\n",
    "####該当するコード\n",
    "<code>\n",
    "    network[key] -= learning_rate * grad[key]\n",
    "</code>\n",
    "\n",
    "### 確率的勾配法(SDG)\n",
    "\n",
    "$ E $の勾配を求める際に全学習データを用いて計算するするのは非常に時間がかかることから、ランダムで学習データを選び、勾配の計算を行う。\n",
    "\n",
    "* 計算コストの軽減\n",
    "* 望まない局所極小値に収束するリスクの軽減\n",
    "* オンライン学習ができる（モデルに都度学習を行わせる）⇔バッチ学習\n",
    "\n",
    "実際のコード\n",
    "<pre>\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    d_batch = d_train[batch_mask]\n",
    "</pre>\n",
    "\n",
    "#### 確認テスト\n",
    "オンライン学習とはデータが入ってくるたびに都度パラメータを更新し、学習させる方法。\n",
    "\n",
    "対して、バッチではすべての学習データを使ってパラメタ更新を行う\n",
    "\n",
    "#### ミニバッチ勾配降下法\n",
    "\n",
    "メモリに乗りきらない巨大なデータを分割して行う。\n",
    "\n",
    "ランダムに分割したデータの集合（ミニバッチ）$D_t$に属するサンプルの平均誤差\n",
    "\n",
    "データを分割して行うことで並列化による高速化が期待できる。\n",
    "\n",
    "#### 確認テスト\n",
    "\n",
    "$$\n",
    "w^{(t+1)}=w^{(t)}-\\epsilon \\nabla E_t\n",
    "$$\n",
    "\n",
    "<img src='BATCH.jpg'>\n",
    "\n",
    "\n",
    "#### 実装演習\n",
    "\n",
    "確率勾配降下法\n",
    "\n",
    "<code>\n",
    "    # データのランダム抽出\n",
    "    random_datasets = np.random.choice(data_sets, epoch)\n",
    "    # 勾配降下の繰り返し\n",
    "    for dataset in random_datasets:\n",
    "    x, d = dataset['x'], dataset['d']\n",
    "    z1, y = forward(network, x)\n",
    "    grad = backward(x, d, z1, y)\n",
    "    # パラメータに勾配適用\n",
    "    for key in ('W1', 'W2', 'b1', 'b2'):\n",
    "        network[key]  -= learning_rate * grad[key]\n",
    "    # 誤差\n",
    "    loss = functions.mean_squared_error(d, y)\n",
    "    losses.append(loss)\n",
    "</code>\n",
    "\n",
    "学習セットのデータすべてを用いずに、\n",
    "<code>\n",
    "        random_datasets = np.random.choice(data_sets, epoch)\n",
    "</code>\n",
    "でデータの一部を取り出し、学習を行っている。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.誤差伝搬法\n",
    "\n",
    "勾配法を用いる際、$ {\\partial E \\over \\partial W} $が必要となる。\n",
    "\n",
    "ところが、数値微分\n",
    "\n",
    "$$\n",
    "{\\partial E \\over \\partial w_n} \\sim { E(w_n + h) - E(w_n - h) \\over 2h }\n",
    "$$\n",
    "\n",
    "は一般的な方法であるが、計算量が非常に大きくなる。\n",
    "\n",
    "このため、ニューラルネットワークでは一般的に誤差逆伝搬法を用いる\n",
    "\n",
    "\n",
    "算出された誤差を出力層から順に微分し、前の層に伝える。\n",
    "\n",
    "<img src=\"BACKWARD_1.jpg\">\n",
    "\n",
    "後ろ層の計算結果から、微分を逆算することで、不要な再帰的な計算を避けることができる。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='BACKWARD_3.jpg'>\n",
    "\n",
    "\n",
    "\n",
    "アフィン変換 $y=x W + b$ の逆伝搬\n",
    "$$\n",
    "{\\partial E \\over \\partial W} = X^T {\\partial E \\over \\partial y} \\\\\n",
    "{\\partial E \\over \\partial X} = {\\partial E \\over \\partial y} W^T \\\\\n",
    "{\\partial E \\over \\partial b} = {\\partial E \\over \\partial y} I\n",
    "$$\n",
    "形をイメージすると覚えやすい\n",
    "<img src='BACKWARD_6.jpg'>\n",
    "\n",
    "ソフトマトリックス出力のクロスエントロピー\n",
    "\n",
    "$$\n",
    "{\\partial E \\over \\partial x}=-\\frac{1}{N}(y - t)\n",
    "$$\n",
    "\n",
    "#### 実装演習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 順伝播\n",
    "def forward(network, x):\n",
    "    print(\"##### 順伝播開始 #####\")\n",
    "\n",
    "    W1, W2 = network['W1'], network['W2']\n",
    "    b1, b2 = network['b1'], network['b2']\n",
    "    \n",
    "    u1 = np.dot(x, W1) + b1\n",
    "    z1 = functions.relu(u1)\n",
    "    u2 = np.dot(z1, W2) + b2\n",
    "    y = functions.softmax(u2)\n",
    "    \n",
    "    print_vec(\"総入力1\", u1)\n",
    "    print_vec(\"中間層出力1\", z1)\n",
    "    print_vec(\"総入力2\", u2)\n",
    "    print_vec(\"出力1\", y)\n",
    "    print(\"出力合計: \" + str(np.sum(y)))\n",
    "\n",
    "    return y, z1\n",
    "\n",
    "# 誤差逆伝播\n",
    "def backward(x, d, z1, y):\n",
    "    print(\"\\n##### 誤差逆伝播開始 #####\")\n",
    "\n",
    "    grad = {}\n",
    "\n",
    "    W1, W2 = network['W1'], network['W2']\n",
    "    b1, b2 = network['b1'], network['b2']\n",
    "    #  出力層でのデルタ\n",
    "    delta2 = functions.d_sigmoid_with_loss(d, y)\n",
    "    #  b2の勾配\n",
    "    grad['b2'] = np.sum(delta2, axis=0)\n",
    "    #  W2の勾配\n",
    "    grad['W2'] = np.dot(z1.T, delta2)\n",
    "    #  中間層でのデルタ\n",
    "    delta1 = np.dot(delta2, W2.T) * functions.d_relu(z1)\n",
    "    # b1の勾配\n",
    "    grad['b1'] = np.sum(delta1, axis=0)\n",
    "    #  W1の勾配\n",
    "    grad['W1'] = np.dot(x.T, delta1)\n",
    "        \n",
    "    print_vec(\"偏微分_dE/du2\", delta2)\n",
    "    print_vec(\"偏微分_dE/du2\", delta1)\n",
    "\n",
    "    print_vec(\"偏微分_重み1\", grad[\"W1\"])\n",
    "    print_vec(\"偏微分_重み2\", grad[\"W2\"])\n",
    "    print_vec(\"偏微分_バイアス1\", grad[\"b1\"])\n",
    "    print_vec(\"偏微分_バイアス2\", grad[\"b2\"])\n",
    "\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記プログラムの backward 関数の解読\n",
    "\n",
    "\n",
    "softmax からの逆伝搬は functions.d_sigmoid_with_loss(d, y) \n",
    "を用いて、$ d_2 $に代入\n",
    "\n",
    "$$\n",
    "u_2 = z_1 \\mathbf{W_2} + b_2 \n",
    "$$\n",
    "\n",
    "であるから、\n",
    "$$\n",
    "{\\partial E \\over \\partial b_2} = [1, 1, \\dots, 1] d_2 \\\\\n",
    "{\\partial E \\over \\mathbf{W_2}} = z_1^T d_2\n",
    "$$\n",
    "同様にして、\n",
    "$$\n",
    "{\\partial E \\over \\partial b_1} = [1, 1, \\dots, 1] d_1 \\\\\n",
    "{\\partial E \\over \\mathbf{W_1}} = x^T d_1\n",
    "$$\n",
    "\n",
    "#### 確認テスト\n",
    "\n",
    "$$\n",
    "{\\partial E \\over \\partial y}{\\partial y \\over \\partial u}\n",
    "$$\n",
    "に相当する部分\n",
    "<code>\n",
    "    delta2 = functions.d_mean_squared_error(d, y)\n",
    " </code>\n",
    " \n",
    " \n",
    "$$\n",
    "{\\partial E \\over \\partial y}{\\partial y \\over \\partial u}{\\partial u \\over \\partial w^{(2)}_p}\n",
    "$$\n",
    "に相当する部分\n",
    "<code>\n",
    "    grad['W2'] = np.dot(z1.T, delta2)\n",
    " </code>\n",
    "\n",
    "#### 実装演習 (1_3_stochastic_gradient_descent)\n",
    "\n",
    "シグモイド関数の場合\n",
    "\n",
    "<img src='BACKWARD_4.jpg'>\n",
    "\n",
    "ReLU 関数の場合\n",
    "<img src='BACKWARD_5.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.勾配消失問題\n",
    "\n",
    "多層構造のネットワークを逆伝搬層で、更新していったとき、勾配が次第に小さくなり、学習が進まなくなる。\n",
    "<img src='VANISHING_3.jpg'>\n",
    "解決法\n",
    "* 活性化関数の選択\n",
    "* 重みの初期化\n",
    "* バッチ正規化\n",
    "\n",
    "### 活性化関数の選択\n",
    "\n",
    "活性化関数 $y=f(u)$とすると、\n",
    "逆伝搬では\n",
    "\n",
    "$$\n",
    "{\\partial E \\over \\partial y}{\\partial y \\over \\partial u}\n",
    "$$\n",
    "\n",
    "のように、活性化関数の微分が掛け合わされ、\n",
    "前層に伝えられる。\n",
    "\n",
    "多くの活性化関数の微分は 0 から 1 の間の値をとり、掛け合わせる毎に小さくなって行く。\n",
    "\n",
    "\n",
    "代表的な例ではシグモイド関数\n",
    "$$\n",
    "y={1 \\over 1 + exp(-x)}\n",
    "$$\n",
    "が上げられる。\n",
    "\n",
    "$$\n",
    "y=f(u)=\\sigma(u)\n",
    "$$\n",
    "とすると、\n",
    "\n",
    "$$\n",
    "\\sigma(u)'=(1-\\sigma(x))\\sigma(x)\n",
    "$$\n",
    "\n",
    "であり、以下のように 0～ 0.25 の値をとる。\n",
    "このため、シグモイドの層を通ると誤差が次第に小さくなり、前方の層に伝わらないということが生ずる。\n",
    "\n",
    "\n",
    "#### 確認テスト\n",
    "\n",
    "シグモイド関数の微分の最大値は 0.25\n",
    "\n",
    "シグモイド関数の微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-10, 10)\n",
    "\n",
    "dy = (1 - sigmoid(x)) * sigmoid(x)\n",
    "plt.plot(x, dy)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU\n",
    "\n",
    "* 勾配消失問題の解消　微分が1なので勾配消失が起こらない\n",
    "* スパース化　必要なところしか伝わらないという利点もある。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初期化方法の改良\n",
    "\n",
    "機械学習を行う場合、重み w の初期値は、適度に散らばっていたほうが望ましい。\n",
    "正規分布で初期化すると、0または1に偏った分布となる。\n",
    "この場合、シグモイド関数等の活性化関数を使った場合、\n",
    "0となってしまい、勾配の消失が発生する。\n",
    "\n",
    "この問題を解決するために、以下のような初期化方法が考案された。\n",
    "\n",
    "* Xavier法\n",
    "\n",
    "前層のノード数が n の場合 $ 1/\\sqrt{n} $を標準偏差とした分布を使う\n",
    "\n",
    "* He法\n",
    "\n",
    "$ \\sqrt{2.0 \\over n} $を標準偏差とした分布を使う\n",
    "\n",
    "### バッチ正規化\n",
    "ミニバッチ内の値の平均を 0、分散を1になるように正規化する。\n",
    "\n",
    "\n",
    "### 実装演習\n",
    "\n",
    "初期化：ガウス分布、活性化関数：シグモイド\n",
    "<img src=\"VANISHING_1.png\">\n",
    "\n",
    "初期化：ガウス分布、活性化関数：ReLU\n",
    "<img src=\"VANISHING_2.png\">\n",
    "\n",
    "初期化：Xavier、活性化関数：シグモイド\n",
    "<img src=\"VANISHING_3.png\">\n",
    "\n",
    "初期化：He、活性化関数：ReLU\n",
    "<img src=\"VANISHING_4.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 学習率最適化手法\n",
    "\n",
    "勾配降下法は以下の計算を行い、E(w)を最小化するwを見つける。\n",
    "$$\n",
    "\\mathbf{w}_{(t+1)} = \\mathbf{w}_{t} - \\epsilon \\nabla E\n",
    "$$\n",
    "\n",
    "学習率の大きさによって\n",
    "\n",
    "* 大きすぎる場合 : 最適値にいつまでも近づかず、振動または、発散\n",
    "* 小さし切る場合：収束までに時間がかかる。大局的最適値に収束しづらい\n",
    "\n",
    "### 勾配法の改良\n",
    "\n",
    "#### Momentum\n",
    "\n",
    "勾配に加えて現在の速度の概念を導入\n",
    "\n",
    "$$\n",
    "\\mathbf{V}_t = \\mu \\mathbf{V}_{t-1} - \\epsilon \\nabla E\n",
    "$$\n",
    "$$\n",
    "\\mathbf{w}^{(t+1)} = \\mathbf{W}^{(t)} + \\mathbf{v_t}\n",
    "$$\n",
    "\n",
    "\n",
    "* 誤差をパラメータで微分したものと学習率の積を減算した後、現在の重みに前回の重みを減算した値と慣性の積を加算する\n",
    "* 局所的最適解にはならず、大域的最適解となる。\n",
    "* 谷間についてから最も低い位置(最適値)にいくまでの時間が早い\n",
    "\n",
    "実際のコード\n",
    "<code>\n",
    "        v[key] = momentum * v[key] - learning_rate * grad[key]\n",
    "        network.params[key] += v[key]\n",
    "</code>\n",
    "\n",
    "#### AdaGrad\n",
    "\n",
    "学習を進めるにつれて学習率を減衰させる方法\n",
    "\n",
    "$$\n",
    "h_0 = \\theta\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_t = h_{t-1} + (\\nabla E)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\epsilon {1 \\over \\sqrt{h_t} + \\theta} \\nabla E \n",
    "$$\n",
    "\n",
    "\n",
    "* 誤差をパラメータで微分したものと再定義した学習率の積を減算する（)\n",
    "* 勾配の緩やかな斜面に対して、最適値に近づける\n",
    "* 学習率が徐々に小さくなるので、鞍点問題を引き起こす事があった。\n",
    "\n",
    "実際のコード\n",
    "<code>\n",
    "        h[key] +=  np.square(grad[key])\n",
    "        network.params[key] -= learning_rate * grad[key] / (np.sqrt(h[key]) + 1e-7)\n",
    "</code>\n",
    "\n",
    "#### RMSProp\n",
    "\n",
    "AdaGrad を改良し、h が大きくなりすぎて、繰り返し回数が大きくなると\n",
    "重みが更されなくなる問題を解消する。\n",
    "\n",
    "\n",
    "$$\n",
    "h_t = \\alpha h_{t-1} + (1-\\alpha)(\\nabla E)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\epsilon {1 \\over \\sqrt{h_t} + \\theta} \\nabla E \n",
    "$$\n",
    "\n",
    "* 局所的最適解にはならず、大域的最適解となる。\n",
    "* ハイパーパラメータの調整が必要な場合が少ない\n",
    "\n",
    "実際のコード\n",
    "<code>\n",
    "        h[key] *= decay_rate\n",
    "        h[key] += (1 - decay_rate) * np.square(grad[key])\n",
    "        network.params[key] -= learning_rate * grad[key] / (np.sqrt(h[key]) + 1e-7)\n",
    "</code>\n",
    "\n",
    "\n",
    "#### Adam\n",
    "\n",
    "MomentumとRMSPropを組み合わせた手法\n",
    "$$\n",
    "\\mathbf{g}^{(t)}=\\nabla E(\\mathbf{w}^{(t)})\\\\\n",
    "$$\n",
    "$$\n",
    "\\mathbf{m}_t=\\rho_1\\mathbf{m}_{t-1}+(1-\\rho_1)\\mathbf{g}^{(t)}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{v}_t=\\rho_2\\mathbf{v}_{t-1}+(1-\\rho_2)(\\mathbf{g}^{(t)})^2\n",
    "$$\n",
    "$$\n",
    "\\hat{\\mathbf{m}}_t=\\frac{\\mathbf{m}_t}{1-\\rho_1^t}\n",
    "$$\n",
    "$$\n",
    "\\hat{\\mathbf{v}}_t=\\frac{\\mathbf{v}_t}{1-\\rho_2^t}\n",
    "$$\n",
    "$$\n",
    "\\Delta\\mathbf{w}^{(t)}=-\\frac{\\eta}{\\sqrt{\\hat{\\mathbf{v}}_t+\\varepsilon}}\\hat{\\mathbf{m}}_t\\\\\n",
    "\\mathbf{w}^{(t+1)}=\\mathbf{w}^{(t)}+\\Delta\\mathbf{w}^{(t)}\n",
    "$$\n",
    "\n",
    "* モメンタムおよびRMSPropのメリットを孕んだアルゴリズムである\n",
    "\n",
    "実際のコード\n",
    "<code>\n",
    "        m[key] += (1 - beta1) * (grad[key] - m[key])\n",
    "        v[key] += (1 - beta2) * (grad[key] ** 2 - v[key])            \n",
    "        network.params[key] -= learning_rate_t * m[key] / (np.sqrt(v[key]) + 1e-7)                \n",
    "</code>\n",
    "\n",
    "\n",
    "それぞれの最適化法の比較\n",
    "\n",
    "<img src='OPTIMIZER_1.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.過学習\n",
    "\n",
    "訓練データに過度に適応してしまい、テストデータ等、その他のデータで精度を得られなくなる。\n",
    "\n",
    "大きなニューラルネットワークで発生する。\n",
    "\n",
    "\n",
    "<img src='OVERTRAINING_1.jpg'>\n",
    "\n",
    "学習の進捗具合をチェックし、過学習を避ける\n",
    "<img src='OVERTRAINING_2.jpg'>\n",
    "\n",
    "過学習を避けるための方法として、\n",
    "\n",
    "* 正規化\n",
    "* ドロップアウト\n",
    "\n",
    "がある。\n",
    "\n",
    "### 正規化\n",
    "\n",
    "誤差\n",
    "$$\n",
    "L=E(w)\n",
    "$$\n",
    "でLを最小化する問題では、過度にテストデータに最適化されてしまう問題が発生する。\n",
    "そこで、Lにペナルティ項（正則化項）を加え、\n",
    "$$\n",
    "L=E(w)+\\frac{1}{p} \\lambda \\|x\\|_p\n",
    "$$\n",
    "を最適化するようにする。\n",
    "\n",
    "ここでは、pノルム\n",
    "$$\n",
    "|x|_p=(\\|x_1\\|^p+\\dots+|x_n|^p)^{1 \\over p}\n",
    "$$\n",
    "を加える。\n",
    "\n",
    "\n",
    "p=1の場合L1正則化 (Lasso回帰） <br/>\n",
    "p=2の場合L2正則化（Ridge回帰）\n",
    "\n",
    "と呼ぶ。\n",
    "\n",
    "\n",
    "#### 参考：AIC(赤池情報量基準)\n",
    "\n",
    "$$\n",
    "AIC = -2 \\log L + 2M\n",
    "$$\n",
    "を最小化するモデルを選ぶ。\n",
    "\n",
    "L 最大尤度\n",
    "\n",
    "M モデルのパラメタ数\n",
    "\n",
    "誤差が正規分布の場合、\n",
    "\n",
    "$$\n",
    "\\log L = {N \\over 2} \\log (2 \\pi \\sigma^2)  - {RSS \\over 2 \\sigma^2}\n",
    "$$\n",
    "\n",
    "RSS : 残差平方和\n",
    "\n",
    "$$\n",
    "\\sigma^2 = {RSS \\over N}\n",
    "$$\n",
    "\n",
    "より、\n",
    "\n",
    "$$\n",
    "\\log L = {N \\over 2} \\log ({2 \\pi RSS \\over N})  - {N \\over 2}\n",
    "$$\n",
    "\n",
    "したがって、\n",
    "$$\n",
    "AIC = N \\log 2\\pi + N \\log {RSS \\over N} + N + 2M\n",
    "$$\n",
    "これより、\n",
    "\n",
    "$$\n",
    "N \\log {RSS \\over N} + 2M\n",
    "$$\n",
    "\n",
    "を最小化することになる。\n",
    "\n",
    "\n",
    "\n",
    "#### 確認テスト\n",
    "\n",
    "L1, L2 正則化の場合、誤差項と正則化項の等高線はそれぞれ、右図、左図になる。\n",
    "L1正則化では、0の所で最小値をとることから、\n",
    "スパース化に貢献する。\n",
    "\n",
    "<img src='VANISHING_5.png'>\n",
    "\n",
    "### ドロップアウト\n",
    "\n",
    "* ノード数が多いことによって生じる弊害を\n",
    "ランダムにノードを削除することによって解決する。\n",
    "* データ量を変化させずに、異なるモデルを学習させていると解釈できる\n",
    "\n",
    "実際のコード\n",
    "<code>\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "</code>\n",
    "\n",
    "ドロップアウト率を決めて、ランダムにノードを返す。\n",
    "\n",
    "\n",
    "#### 実装演習\n",
    "\n",
    "過学習\n",
    "<img src='OVERFITTING_1.jpg'>\n",
    "\n",
    "L2正則化\n",
    "<img src='OVERFITTING_2.jpg'>\n",
    "\n",
    "L1正則化 weight_decay_lambda=0.005\n",
    "<img src='OVERFITTING_3.jpg'>\n",
    "\n",
    "L1正則化 weight_decay_lambda=0.05\n",
    "<img src='OVERFITTING_4.jpg'>\n",
    "\n",
    "L1正則化 weight_decay_lambda=0.0005\n",
    "<img src='OVERFITTING_5.jpg'>\n",
    "\n",
    "ドロップアウト dropout_ratio=0.15\n",
    "<img src='OVERFITTING_6.jpg'>\n",
    "\n",
    "ドロップアウト dropout_ratio=0.05\n",
    "<img src='OVERFITTING_7.jpg'>\n",
    "\n",
    "ドロップアウト dropout_ratio=0.3\n",
    "<img src='OVERFITTING_8.jpg'>\n",
    "\n",
    "ドロップアウト + L1正則化\n",
    "<img src='OVERFITTING_9.jpg'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.畳み込みニューラルネットワークの概念\n",
    "\n",
    "畳み込みニューラルネットワークは、次元的にでつながりあるデータ、音声や画像等データ等を取り扱うためによく使われている。\n",
    "\n",
    "\n",
    "\n",
    "### 畳み込みの概念\n",
    "<img src=\"CONVOLUTION_1.jpg\">\n",
    "\n",
    "フィルタを動かしながら、入力データと掛け合わせ、合計を求める。\n",
    "\n",
    "$$\n",
    "u(i,j,k)=\\sum_m \\sum_n x(i+m, j+n)h(m,n, k)\n",
    "$$\n",
    "\n",
    "\n",
    "* FFTやデジタルフィルタの処理に似ている。\n",
    "* 近接したニューロンの情報を取り込む。→急激に画像の変化する場所等を見つけるのに役立つ\n",
    "\n",
    "### パディング\n",
    "入力データ（画像）の端の部分をどのように処理するかという方法。\n",
    "この例では、拡張した部分を 0 として処理している。\n",
    "\n",
    "<img src=\"PADDING_1.jpg\">\n",
    "\n",
    "### ストライド\n",
    "フィルタをどれだけ動かすか\n",
    "\n",
    "<img src=\"STRIDE_1.jpg\">\n",
    "\n",
    "以上の操作により入力画像の次元$(H,W)$に対し、\n",
    "* フィルタサイズ $(F_h, F_w)$\n",
    "* ストライドS\n",
    "* パッディング P \n",
    "\n",
    "を適用した場合、出力画像の次元$(O_h, O_w)$ は\n",
    "\n",
    "$$\n",
    "O_h = {H + 2P - F_h \\over S } + 1\n",
    "$$\n",
    "$$\n",
    "O_h = {W + 2P - F_w \\over S } + 1\n",
    "$$\n",
    "\n",
    "となる。\n",
    "\n",
    "\n",
    "#### 確認テスト\n",
    "\n",
    "\n",
    "サイズ6×6の入力画像を、サイズ2×2のフィルタで畳み込んだ時の出力画像のサイズを答えよ。なおストライドとパディングは1とする。\n",
    "\n",
    "$$\n",
    "(6 + 2 - 2 )/ 1 + 1 = 7\n",
    "$$\n",
    "\n",
    "答え： 7 x 7\n",
    "\n",
    "### チャンネル\n",
    "\n",
    "ある次元の座標における情報の数。\n",
    "\n",
    "* モノラル音声：1チャンネル\n",
    "* カラー画像：RGB 3チャンネル\n",
    "* CTスキャン : 1 チャンネル\n",
    "\n",
    "<img src='CHANNEL_1.png'>\n",
    "\n",
    "### プーリング\n",
    "\n",
    "矩形領域で最大値や平均値等をとり(Max Pooling, Average Pooling)、出力座標の値とする。\n",
    "\n",
    "<img src='POOLING_1.jpg'>\n",
    "\n",
    "\n",
    "### 全結合で学習した際の課題\n",
    "\n",
    "画像の場合、縦、横、チャンネルの3次元データだが、1次元のデータとして処理される。→RGBの各チャンネル間の関連性が、学習に反映されないということ。\n",
    "\n",
    "#### 実習演習\n",
    "\n",
    "畳み込みの計算を行うために、im2col で計算しやすい形に変換\n",
    "<code>\n",
    "        for y in range(filter_h):\n",
    "        y_max = y + stride * out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride * out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "</code>\n",
    "\n",
    "矩形範囲の要素を行ベクトルに展開する\n",
    "\n",
    "実行例\n",
    "\n",
    "\t========== input_data ===========\n",
    "\t [[[[87. 22. 43. 62.]\n",
    "\t   [66. 31. 56.  0.]\n",
    "\t   [55. 98. 63. 30.]\n",
    "\t   [18. 25. 73. 84.]]]\n",
    "\t\n",
    "\t\n",
    "\t [[[40. 89. 10. 64.]\n",
    "\t   [48. 94. 25. 92.]\n",
    "\t   [27. 14. 85. 57.]\n",
    "\t   [57. 72. 87. 43.]]]]\n",
    "\t==============================\n",
    "\t============= col ==============\n",
    "\t [[87. 22. 43. 66. 31. 56. 55. 98. 63.]\n",
    "\t [22. 43. 62. 31. 56.  0. 98. 63. 30.]\n",
    "\t [66. 31. 56. 55. 98. 63. 18. 25. 73.]\n",
    "\t [31. 56.  0. 98. 63. 30. 25. 73. 84.]\n",
    "\t [40. 89. 10. 48. 94. 25. 27. 14. 85.]\n",
    "\t [89. 10. 64. 94. 25. 92. 14. 85. 57.]\n",
    "\t [48. 94. 25. 27. 14. 85. 57. 72. 87.]\n",
    "\t [94. 25. 92. 14. 85. 57. 72. 87. 43.]]\n",
    "\t==============================\n",
    "\n",
    "実際の畳み込み処理\n",
    "\n",
    "順伝搬\n",
    "<code>\n",
    "    def forward(self, x):\n",
    "        # FN: filter_number, C: channel, FH: filter_height, FW: filter_width\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        # 出力値のheight, width\n",
    "        out_h = 1 + int((H + 2 * self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2 * self.pad - FW) / self.stride)        \n",
    "        # xを行列に変換\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        # フィルターをxに合わせた行列に変換\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        # 計算のために変えた形式を戻す\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "        return out\n",
    "</code>\n",
    "\n",
    "逆伝搬\n",
    "\n",
    "<code>\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        # dcolを画像データに変換\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "        return dx\n",
    " </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.最新のCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet\n",
    "\n",
    "ILSVRC-2012 で優勝。前年度の誤答率 26% を 16% までに改善\n",
    "\n",
    "当時、勾配消失の問題で、ネットワークを深くしても\n",
    "良い結果が得られなかった問題を、ReLUを利用し、\n",
    "深いネットワークを作ることで、それまでニューラルネットでは\n",
    "良い成績を上げられなかった画像認識の分野で革新を起こした。\n",
    "\n",
    "### ILSVRC-2012\n",
    "* 256 x 256 のカラー画像 1000 分類\n",
    "* 1.2M 訓練データ\n",
    "* 50k 検証用\n",
    "\n",
    "### AlexNet\n",
    "\n",
    "\n",
    "<img src='ALEXNET_1.jpg'>\n",
    "\n",
    "* ReLU の利用 (ReLU 自体は 2010年に登場)\n",
    "* dropout の利用\n",
    "* GPUの利用\n",
    "\n",
    "* 2列のネットを利用(当時の計算機の制限で並列計算を行う）\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
